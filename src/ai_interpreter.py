import aiohttp
import json
import logging
import asyncio
import re
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import os

logger = logging.getLogger(__name__)

try:
    # –æ—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å ‚Äî –∫–æ–≥–¥–∞ –≤–µ—Å—å –º–æ–Ω–æ—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –¥–æ—Å—Ç—É–ø–µ–Ω
    from src.config import OPENROUTER_CONFIG, get_available_models  # type: ignore
except ImportError as e:
    logger.warning(
        "ai_interpreter: cannot import src.config, falling back to ENV-only config: %s",
        e,
    )

    class _EnvOpenRouterConfig:
        def __init__(self) -> None:
            self.api_key = os.getenv("OPENROUTER_API_KEY", "")
            self.base_url = os.getenv(
                "OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1"
            )
            self.max_tokens = int(os.getenv("OPENROUTER_MAX_TOKENS", "1000"))
            self.temperature = float(os.getenv("OPENROUTER_TEMPERATURE", "0.7"))
            self.timeout = int(os.getenv("OPENROUTER_TIMEOUT", "30"))
            # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ‚Äî –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å max_retries, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ:
            self.max_retries = int(os.getenv("OPENROUTER_MAX_RETRIES", "2"))

    OPENROUTER_CONFIG = _EnvOpenRouterConfig()

    def get_available_models() -> list[str]:
        raw = os.getenv("OPENROUTER_MODELS", "")
        models = [m.strip() for m in raw.split(",") if m.strip()]
        if models:
            return models

        # –∂—ë—Å—Ç–∫–∏–π fallback –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –≤–æ–æ–±—â–µ —á—Ç–æ-—Ç–æ —Ä–∞–±–æ—Ç–∞–ª–æ
        return [
            "meta-llama/llama-3.3-70b-instruct:free",
            "google/gemma-2-9b-it:free",
            "qwen/qwen-2-7b-instruct:free",
        ]

from .ai_prompts import (
    BASE_TAROT_SYSTEM_PROMPT,
    build_profile_context,
    build_spread_interpretation_prompt,
    build_question_answer_prompt,
)

# ‚úÖ –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ì–ï–†–ê: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
logger.propagate = False  # ‚úÖ –ó–ê–ü–†–ï–¢ –î–£–ë–õ–ò–†–û–í–ê–ù–ò–Ø –õ–û–ì–û–í

class AIInterpreter:
    def __init__(self):
        self.api_key = OPENROUTER_CONFIG.api_key

        # ‚úÖ –ò–°–¢–û–ß–ù–ò–ö –ú–û–î–ï–õ–ï–ô: –±–µ—Ä—ë–º –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        models = []
        try:
            models = get_available_models()
        except Exception as e:
            logger.error(f"‚ùå Failed to load models from config: {e}")
            models = []

        self.model_list = models or []
        if not self.model_list:
            logger.error("üö® CRITICAL: model_list is empty! Using fallback meta-llama")
            self.model_list = ["meta-llama/llama-3.3-70b-instruct"]

        # ‚úÖ –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –í–´–°–û–ö–û–ì–û –£–†–û–í–ù–Ø: —Ç–æ–ª—å–∫–æ –ø–æ—Ä—è–¥–æ–∫ –º–æ–¥–µ–ª–µ–π
        model_names = [m.split('/')[-1] for m in self.model_list]
        logger.info(f"üîß AIInterpreter model_list order: {model_names}")

        self.base_url = OPENROUTER_CONFIG.base_url
        self.max_tokens = OPENROUTER_CONFIG.max_tokens
        self.temperature = 1.0

        # ‚úÖ PER-MODEL –¢–ê–ô–ú–ê–£–¢–´
        self.request_timeout = getattr(
            OPENROUTER_CONFIG, "timeout", 60
        )  # –ë–∞–∑–æ–≤—ã–π —Ç–∞–π–º–∞—É—Ç 60 —Å–µ–∫—É–Ω–¥
        self.per_model_timeout = {
            "meta-llama/llama-3.3-70b-instruct": 90,  # 90 —Å–µ–∫—É–Ω–¥ –¥–ª—è —Ç—è–∂–µ–ª–æ–π –º–æ–¥–µ–ª–∏
            "microsoft/wizardlm-2-8x22b:free": 90,  # 90 —Å–µ–∫—É–Ω–¥ –¥–ª—è –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏
        }

        # ‚úÖ –£–°–û–í–ï–†–®–ï–ù–°–¢–í–û–í–ê–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø RETRY/BACKOFF
        self.max_retries = 2
        self.base_backoff = 1.5
        self.backoff_multiplier = 1.5
        self.max_backoff = 3.0  # ‚úÖ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ó–ê–î–ï–†–ñ–ö–ê 3 –°–ï–ö–£–ù–î–´

        logger.info(
            f"‚è±Ô∏è Request timeout: base={self.request_timeout}s, meta-llama=90s"
        )
        logger.info(
            f"üîÑ Retry config: {self.max_retries} attempts, backoff: {self.base_backoff}‚Üí{self.max_backoff}s"
        )

        # Circuit breaker state
        self._model_failures: Dict[str, int] = {}
        self._model_cooldown_until: Dict[str, float] = {}
        self._model_cooldown_duration = 300

        # Session cache for successful models
        self._preferred_models: Dict[int, Tuple[str, float]] = {}
        self._preferred_model_ttl = 1800

        self._validate_parameters()
        # prompt_cache –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–∞ –±—É–¥—É—â–µ–µ/—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å, –Ω–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –≤ ai_prompts
        self.prompt_cache: Dict[str, str] = {}
        self.cache_size = 50

        logger.info(
            f"‚úÖ AI Interpreter initialized with {len(self.model_list)} models"
        )

    def _validate_parameters(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        if not (0 <= self.temperature <= 2):
            logger.warning(
                f"‚ö†Ô∏è Invalid temperature {self.temperature}, clamping to 1.0"
            )
            self.temperature = 1.0

        if self.max_tokens > 4000:
            logger.warning(f"‚ö†Ô∏è High max_tokens {self.max_tokens}, clamping to 4000")
            self.max_tokens = 4000

    def _get_request_timeout(self, model: str) -> int:
        """‚úÖ –ü–û–õ–£–ß–ï–ù–ò–ï –¢–ê–ô–ú–ê–£–¢–ê –î–õ–Ø –ö–û–ù–ö–†–ï–¢–ù–û–ô –ú–û–î–ï–õ–ò"""
        return self.per_model_timeout.get(model, self.request_timeout)

    def _calculate_backoff(self, attempt: int) -> float:
        """‚úÖ –†–ê–°–ß–ï–¢ BACKOFF –° –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï–ú –ú–ê–ö–°–ò–ú–£–ú–ê"""
        backoff = self.base_backoff * (self.backoff_multiplier ** attempt)
        return min(backoff, self.max_backoff)

    async def generate_interpretation(
        self,
        spread_type: str,
        cards: list,
        category: str,
        user_age: int = None,
        user_gender: str = None,
        user_name: str = None,
        user_id: Optional[int] = None,
        model: str = None,
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ä–∞—Å–∫–ª–∞–¥–∞

        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º: {success, text, model, error}
        """
        try:
            logger.info(
                f"üéØ Generating interpretation for {len(cards)} cards, category: {category}"
            )

            # ‚úÖ DEBUG: –ª–æ–≥–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫ –º–æ–¥–µ–ª–µ–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if logger.isEnabledFor(logging.DEBUG):
                model_names = [m.split("/")[-1] for m in self.model_list]
                logger.debug(f"üîß Current model_list order: {model_names}")

            profile_context = build_profile_context(
                user_age=user_age, user_gender=user_gender, user_name=user_name
            )
            spread_data = {
                "spread_type": spread_type,
                "cards": cards,
            }

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π –±–∏–ª–¥–µ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤
            prompt = build_spread_interpretation_prompt(
                spread_type=spread_type,
                cards=cards,
                question_category=category,
                profile_context=profile_context,
            )
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"üìù Prompt length: {len(prompt)} characters")

            # ‚úÖ –ï–°–õ–ò –ü–ï–†–ï–î–ê–ù–ê –ö–û–ù–ö–†–ï–¢–ù–ê–Ø –ú–û–î–ï–õ–¨ - –ò–°–ü–û–õ–¨–ó–£–ï–ú –¢–û–õ–¨–ö–û –ï–Å
            if model:
                logger.info(f"üéØ Using specific model: {model}")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤ cooldown –ª–∏ –º–æ–¥–µ–ª—å
                if self._is_model_in_cooldown(model):
                    logger.warning(f"‚è∏Ô∏è Model {model} is in cooldown")
                    return {
                        "success": False,
                        "text": None,
                        "model": model,
                        "error": f"Model {model} is temporarily unavailable",
                    }

                result = await self._make_llm_request(
                    model=model,
                    spread_data=spread_data,
                    question_category=category,
                    profile_context=profile_context,
                )

                if result["success"] and self._is_valid_interpretation(result["text"]):
                    logger.info(f"‚úÖ SUCCESS with model {model}")
                    self._record_model_success(model)

                    cleaned_response = self._clean_response(result["text"])
                    final_response = self._clean_ai_response(cleaned_response)

                    return {
                        "success": True,
                        "text": final_response,
                        "model": model,
                        "error": None,
                    }
                else:
                    logger.warning(
                        f"‚ùå Model {model} failed: {result.get('error', 'Unknown error')}"
                    )
                    self._record_model_failure(model)
                    return result

            # ‚úÖ –°–¢–ê–ù–î–ê–†–¢–ù–ê–Ø –õ–û–ì–ò–ö–ê –° –ö–≠–®–ï–ú –ò CIRCUIT BREAKER
            return await self._generate_with_fallback(
                spread_data=spread_data,
                category=category,
                profile_context=profile_context,
                user_id=user_id,
            )

        except Exception as e:
            logger.error(f"‚ùå Unexpected error in generate_interpretation: {e}")
            return {
                "success": False,
                "text": None,
                "model": model,
                "error": f"Unexpected error: {str(e)}",
            }

    async def _generate_with_fallback(
        self,
        spread_data: Dict,
        category: str,
        profile_context: str,
        user_id: Optional[int] = None,
    ) -> Dict[str, Any]:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å –∫—ç—à–µ–º –∏ circuit breaker"""
        preferred_model = self._get_preferred_model(user_id)
        models_to_try = self.model_list.copy()

        # ‚úÖ –ó–ê–ü–†–ï–¢ –ù–ê –ü–ï–†–ï–ú–ï–©–ï–ù–ò–ï DEEPSEEK –í –ù–ê–ß–ê–õ–û
        if (
            preferred_model
            and preferred_model in models_to_try
            and preferred_model != "deepseek/deepseek-r1:free"
        ):
            models_to_try.remove(preferred_model)
            models_to_try.insert(0, preferred_model)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"üéØ Starting with preferred model: {preferred_model}")

        # –ü–µ—Ä–µ–±–æ—Ä –º–æ–¥–µ–ª–µ–π —Å circuit breaker
        for i, model in enumerate(models_to_try, 1):
            if self._is_model_in_cooldown(model):
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"‚è∏Ô∏è Skipping model in cooldown: {model}")
                continue

            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"üîÑ Trying model {i}/{len(models_to_try)}: {model}")

            result = await self._make_llm_request(
                model=model,
                spread_data=spread_data,
                question_category=category,
                profile_context=profile_context,
            )

            if result["success"] and self._is_valid_interpretation(result["text"]):
                logger.info(f"‚úÖ SUCCESS with model {model}")

                # ‚úÖ –ó–ê–ü–†–ï–¢ –ù–ê –ö–≠–®–ò–†–û–í–ê–ù–ò–ï DEEPSEEK
                if model != "deepseek/deepseek-r1:free":
                    self._set_preferred_model(user_id, model)

                self._record_model_success(model)

                cleaned_response = self._clean_response(result["text"])
                final_response = self._clean_ai_response(cleaned_response)

                return {
                    "success": True,
                    "text": final_response,
                    "model": model,
                    "error": None,
                }
            else:
                logger.warning(
                    f"‚ùå Model {model} failed: {result.get('error', 'Unknown error')}"
                )
                self._record_model_failure(model)
                continue

        # –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
        logger.error("‚ùå All AI models failed to generate interpretation")
        fallback_text = self._generate_basic_interpretation(spread_data, category)

        return {
            "success": False,
            "text": fallback_text,
            "model": None,
            "error": "All models failed to generate valid interpretation",
        }

    async def _make_llm_request(
        self,
        model: str,
        prompt: Optional[str] = None,
        spread_data: Optional[Dict] = None,
        question_category: Optional[str] = None,
        profile_context: str = "",
    ) -> Dict[str, Any]:
        """‚úÖ –£–°–û–í–ï–†–®–ï–ù–°–¢–í–û–í–ê–ù–ù–´–ô –ú–ï–¢–û–î –ó–ê–ü–†–û–°–ê"""

        if prompt is None:
            if spread_data is None or question_category is None:
                return {
                    "success": False,
                    "text": None,
                    "model": model,
                    "error": "Missing required parameters for prompt generation",
                }

            spread_type = spread_data.get("spread_type", "unknown")
            cards = spread_data.get("cards", [])

            prompt = build_spread_interpretation_prompt(
                spread_type=spread_type,
                cards=cards,
                question_category=question_category,
                profile_context=profile_context,
            )

        # ‚úÖ System prompt —Ç–µ–ø–µ—Ä—å –±–µ—Ä—ë–º –∏–∑ –æ–±—â–µ–≥–æ –º–æ–¥—É–ª—è
        system_prompt = BASE_TAROT_SYSTEM_PROMPT

        payload = {
            "model": model,
            "messages": [
                {
                    "role": "system",
                    "content": system_prompt.strip(),
                },
                {
                    "role": "user",
                    "content": prompt,
                },
            ],
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "stream": False,
        }

        payload = self._validate_payload(payload)

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://tarot-bot-luna.com",
            "X-Title": "Tarot Bot Luna",
        }

        # ‚úÖ –£–°–û–í–ï–†–®–ï–ù–°–¢–í–û–í–ê–ù–ù–´–ô BACKOFF –° –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï–ú
        for attempt in range(self.max_retries):
            start_time = time.time()
            try:
                timeout_seconds = self._get_request_timeout(model)
                timeout = aiohttp.ClientTimeout(total=timeout_seconds)

                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"üì§ Sending request to {model}, attempt {attempt + 1}, timeout: {timeout_seconds}s"
                    )

                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.base_url}/chat/completions",
                        headers=headers,
                        json=payload,
                        timeout=timeout,
                    ) as response:
                        end_time = time.time()
                        elapsed = end_time - start_time
                        response_headers = dict(response.headers)

                        # ‚úÖ DEBUG: –ª–æ–≥–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç–∞
                        if logger.isEnabledFor(logging.DEBUG):
                            logger.debug(
                                f"üì® Response (model={model}) status={response.status} time={elapsed:.1f}s"
                            )
                            logger.debug(f"üîß Response headers: {response_headers}")

                        if response.status == 200:
                            raw_body = await response.text()

                            # ‚úÖ DEBUG: –ª–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–µ —Ç–µ–ª–æ –æ—Ç–≤–µ—Ç–∞
                            if logger.isEnabledFor(logging.DEBUG):
                                logger.debug(
                                    f"üìÑ Raw response body (model={model}): {raw_body[:2000]!r}"
                                )

                            try:
                                result = json.loads(raw_body)
                                interpretation = (
                                    result["choices"][0]["message"]["content"].strip()
                                )

                                # ‚úÖ INFO: —Ç–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                                logger.info(
                                    f"‚úÖ SUCCESS: {model} responded in {elapsed:.1f}s, len={len(interpretation)}"
                                )

                                return {
                                    "success": True,
                                    "text": interpretation,
                                    "model": model,
                                    "error": None,
                                }
                            except (
                                json.JSONDecodeError,
                                KeyError,
                                IndexError,
                            ) as e:
                                logger.error(
                                    f"‚ùå Failed to parse response from {model}: {str(e)}"
                                )
                                return {
                                    "success": False,
                                    "text": None,
                                    "model": model,
                                    "error": f"Failed to parse API response: {str(e)}",
                                }

                        else:
                            error_text = await response.text()
                            end_time = time.time()
                            elapsed = end_time - start_time

                            logger.error(
                                f"‚ùå API Error {response.status} for {model}: {error_text}"
                            )

                            if response.status == 429:
                                retry_after = response_headers.get("Retry-After")
                                wait_time = self._calculate_backoff(attempt)

                                if retry_after:
                                    try:
                                        wait_time = min(
                                            int(retry_after), self.max_backoff
                                        )
                                        if logger.isEnabledFor(logging.DEBUG):
                                            logger.debug(
                                                f"‚è∞ Using Retry-After header: {wait_time} seconds"
                                            )
                                    except ValueError:
                                        logger.warning(
                                            f"‚ö†Ô∏è Invalid Retry-After header: {retry_after}"
                                        )

                                logger.warning(
                                    f"‚è≥ Rate limit hit for {model}. Waiting {wait_time:.1f} seconds..."
                                )
                                await asyncio.sleep(wait_time)
                                continue

                            return {
                                "success": False,
                                "text": None,
                                "model": model,
                                "error": f"API returned status {response.status}: {error_text[:200]}",
                            }

            except asyncio.TimeoutError:
                end_time = time.time()
                elapsed = end_time - start_time
                timeout_setting = self._get_request_timeout(model)

                logger.warning(
                    f"‚è∞ Request timeout (model={model}) after {elapsed:.1f}s (timeout setting: {timeout_setting}s)"
                )

                if attempt == self.max_retries - 1:
                    return {
                        "success": False,
                        "text": None,
                        "model": model,
                        "error": f"Timeout after {self.max_retries} attempts",
                    }

                wait_time = self._calculate_backoff(attempt)
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"‚è≥ Waiting {wait_time:.1f}s before retry after timeout..."
                    )
                await asyncio.sleep(wait_time)

            except Exception as e:
                end_time = time.time()
                elapsed = end_time - start_time

                logger.error(
                    f"‚ùå Model {model} error on attempt {attempt + 1}: {str(e)}"
                )

                if attempt == self.max_retries - 1:
                    return {
                        "success": False,
                        "text": None,
                        "model": model,
                        "error": f"Exception after {self.max_retries} attempts: {str(e)}",
                    }

                wait_time = self._calculate_backoff(attempt)
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"‚è≥ Waiting {wait_time:.1f}s before retry after exception..."
                    )
                await asyncio.sleep(wait_time)

        return {
            "success": False,
            "text": None,
            "model": model,
            "error": f"All {self.max_retries} attempts failed",
        }

    def _validate_payload(self, payload: Dict) -> Dict:
        """–ó–∞—â–∏—Ç–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è payload –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º"""
        validated_payload = payload.copy()

        temp = validated_payload.get("temperature", self.temperature)
        if not (0 <= temp <= 2):
            logger.warning(f"üö® Invalid temperature {temp}, clamping to 1.0")
            validated_payload["temperature"] = 1.0

        tokens = validated_payload.get("max_tokens", self.max_tokens)
        if tokens > 4000:
            logger.warning(f"üö® High max_tokens {tokens}, clamping to 4000")
            validated_payload["max_tokens"] = 4000

        return validated_payload

    def _is_model_in_cooldown(self, model: str) -> bool:
        """Circuit breaker: –ü—Ä–æ–≤–µ—Ä–∫–∞ cooldown –¥–ª—è –º–æ–¥–µ–ª–∏"""
        if model not in self._model_cooldown_until:
            return False

        cooldown_until = self._model_cooldown_until[model]
        if time.time() < cooldown_until:
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(
                    f"üîí Model {model} in cooldown until {datetime.fromtimestamp(cooldown_until)}"
                )
            return True

        del self._model_cooldown_until[model]
        if model in self._model_failures:
            del self._model_failures[model]
        return False

    def _record_model_failure(self, model: str):
        """Circuit breaker: –ó–∞–ø–∏—Å—å –Ω–µ—É–¥–∞—á–∏"""
        current_failures = self._model_failures.get(model, 0) + 1
        self._model_failures[model] = current_failures

        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"üìâ Model {model} failure count: {current_failures}")

        if current_failures >= 3:
            cooldown_until = time.time() + self._model_cooldown_duration
            self._model_cooldown_until[model] = cooldown_until
            logger.warning(
                f"üö® Model {model} entering cooldown until {datetime.fromtimestamp(cooldown_until)}"
            )

    def _record_model_success(self, model: str):
        """Circuit breaker: –°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –Ω–µ—É–¥–∞—á"""
        if model in self._model_failures:
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"üîÑ Resetting failure count for model {model}")
            del self._model_failures[model]

    def _get_preferred_model(self, user_id: Optional[int] = None) -> Optional[str]:
        """–ö—ç—à —É—Å–ø–µ—à–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if not user_id:
            return None

        if user_id in self._preferred_models:
            model, expiry_ts = self._preferred_models[user_id]
            if time.time() < expiry_ts and not self._is_model_in_cooldown(model):
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"üéØ Using preferred model {model} for user {user_id}"
                    )
                return model
            else:
                del self._preferred_models[user_id]

        return None

    def _set_preferred_model(self, user_id: Optional[int], model: str):
        """–ö—ç—à —É—Å–ø–µ—à–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if user_id:
            expiry_ts = time.time() + self._preferred_model_ttl
            self._preferred_models[user_id] = (model, expiry_ts)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(
                    f"üíæ Cached preferred model {model} for user {user_id}"
                )

    def _contains_english_text(self, text: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —è–∑—ã–∫–∞: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return False

        english_word_pattern = re.compile(r"\b[a-zA-Z]{3,}\b")
        english_words = english_word_pattern.findall(text)

        if len(english_words) >= 2:
            logger.warning(
                f"üö® Detected English words in response: {english_words[:3]}"
            )
            return True

        return False

    def _is_valid_interpretation(self, interpretation: str) -> bool:
        """–£—Å–∏–ª–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —è–∑—ã–∫–∞"""
        if not interpretation or len(interpretation.strip()) < 50:
            logger.warning("‚ùå Invalid interpretation: too short")
            return False

        interpretation_lower = interpretation.lower()

        forbidden_phrases = [
            "provide me with more context",
            "could you please provide",
            "what would you like me to do",
            "i need more information",
            "please provide",
            "tell me more",
            "–∫–∞–∫—É—é –∑–∞–¥–∞—á—É",
            "—á—Ç–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ",
            "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ",
            "—É—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞",
            "–∫–∞–∫ —Ç–∞—Ä–æ–ª–æ–≥, —è",
            "–≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ç–∞—Ä–æ–ª–æ–≥–∞",
            "—Å–æ–≥–ª–∞—Å–Ω–æ –∫–∞—Ä—Ç–∞–º —Ç–∞—Ä–æ",
        ]

        for phrase in forbidden_phrases:
            if phrase in interpretation_lower:
                logger.warning(
                    f"‚ùå Invalid interpretation - contains forbidden phrase: {phrase}"
                )
                return False

        if self._contains_english_text(interpretation):
            logger.warning("‚ùå Invalid interpretation - contains English text")
            return False

        if interpretation_lower.count("?") > 2:
            logger.warning("‚ùå Invalid interpretation - too many questions")
            return False

        if len(interpretation.split()) < 30:
            logger.warning("‚ùå Invalid interpretation - too few words")
            return False

        return True

    def _clean_ai_response(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ AI-–æ—Ç–≤–µ—Ç–∞ –æ—Ç –∞–Ω–≥–ª–∏—Ü–∏–∑–º–æ–≤ –∏ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ç–µ–≥–æ–≤ –≤—Ä–æ–¥–µ <think>"""
        if not text:
            return text

        # ‚úÖ –£–¥–∞–ª—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–æ–Ω–æ–ª–æ–≥ reasoning-–º–æ–¥–µ–ª–µ–π (<think>...</think>)
        if "<think>" in text:
            if "</think>" in text:
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –∏–¥—ë—Ç –ü–û–°–õ–ï –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–≥–æ —Ç–µ–≥–∞ </think>
                text = text.split("</think>", 1)[1]
            else:
                # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –≤—ã—Ä–µ–∑–∞–µ–º –≤—Å—ë, —á—Ç–æ –Ω–∞—á–∏–Ω–∞—è —Å <think>
                text = re.sub(r"<think>.*", "", text, flags=re.DOTALL)

        # –ß–∏—Å—Ç–∏–º –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Ç–µ–≥–∏, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –æ—Å—Ç–∞–ª–∏—Å—å
        text = re.sub(r"</?think>", "", text, flags=re.IGNORECASE)

        # –û–±—Ä–µ–∑–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏/–ø—Ä–æ–±–µ–ª—ã
        text = text.strip()

        corrections = {
            "responsable": "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π",
            "stable": "—Å—Ç–∞–±–∏–ª—å–Ω—ã–π",
            "energy": "—ç–Ω–µ—Ä–≥–∏—è",
            "card": "–∫–∞—Ä—Ç–∞",
            "spread": "—Ä–∞—Å–∫–ª–∞–¥",
            "upright": "–ø—Ä—è–º–∞—è",
            "reversed": "–ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç–∞—è",
            "tarot": "—Ç–∞—Ä–æ",
            "reading": "–≥–∞–¥–∞–Ω–∏–µ",
            "interpretation": "—Ç–æ–ª–∫–æ–≤–∞–Ω–∏–µ",
            "advice": "—Å–æ–≤–µ—Ç",
            "guidance": "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
            "message": "–ø–æ—Å–ª–∞–Ω–∏–µ",
        }

        for wrong, correct in corrections.items():
            text = text.replace(wrong, correct)
            text = text.replace(wrong.capitalize(), correct.capitalize())

        return text

    def _clean_response(self, response: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"""
        if not response:
            return response

        clean_phrases = [
            "–ö–æ–Ω–µ—á–Ω–æ, –≤–æ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:",
            "–í–æ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–∞—à–µ–≥–æ —Ä–∞—Å–∫–ª–∞–¥–∞:",
            "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–∞—Ä—Ç:",
            "–í–æ—Ç —á—Ç–æ –≥–æ–≤–æ—Ä—è—Ç –∫–∞—Ä—Ç–∞:",
            "–ö–∞—Ä—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç:",
            "–ù–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–µ–≥–æ —Ä–∞—Å–∫–ª–∞–¥–∞:",
        ]

        for phrase in clean_phrases:
            response = response.replace(phrase, "")

        response = response.strip()

        if len(response) > 2000:
            response = response[:2000] + "..."

        return response

    def _generate_basic_interpretation(
        self, spread_data: dict, question_category: str
    ) -> str:
        """–õ–æ–∫–∞–ª—å–Ω—ã–π fallback"""
        cards = spread_data["cards"]
        spread_type = spread_data["spread_type"]

        if spread_type == "1 –∫–∞—Ä—Ç–∞":
            card = cards[0]

            if isinstance(card, dict):
                card_name = card["name"]
                is_reversed = card.get("is_reversed", False)
            else:
                card_name = card.name
                is_reversed = getattr(card, "is_reversed", False)

            interpretation = (
                f"‚ú® {card_name} ({'–ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç–∞—è' if is_reversed else '–ø—Ä—è–º–∞—è'})\n\n"
            )
            interpretation += (
                f"–í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ {question_category} —ç—Ç–∞ –∫–∞—Ä—Ç–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –≤–∞–∂–Ω—ã–π –∞—Å–ø–µ–∫—Ç —Ç–≤–æ–µ–π –∂–∏–∑–Ω–∏."
            )

        else:
            positions = spread_data.get(
                "positions", ["–ü—Ä–æ—à–ª–æ–µ", "–ù–∞—Å—Ç–æ—è—â–µ–µ", "–ë—É–¥—É—â–µ–µ"]
            )
            interpretation = "üîÆ –†–∞—Å–∫–ª–∞–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —ç—Ç–∞–ø–∞–º–∏:\n\n"

            for i, card in enumerate(cards):
                if isinstance(card, dict):
                    card_name = card["name"]
                else:
                    card_name = card.name

                position = (
                    positions[i] if i < len(positions) else f"–ü–æ–∑–∏—Ü–∏—è {i + 1}"
                )
                interpretation += f"‚Ä¢ {position}: {card_name}\n"

            interpretation += (
                f"\n–û–±—â–∞—è —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è –≤ —Å—Ñ–µ—Ä–µ {question_category}."
            )

        interpretation += (
            "\n\n–î–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ø–æ–ø—Ä–æ–±—É–π —Å–¥–µ–ª–∞—Ç—å —Ä–∞—Å–∫–ª–∞–¥ –µ—â–µ —Ä–∞–∑."
        )
        return interpretation

    async def generate_question_answer(
        self,
        spread_id: int,
        user_id: int,
        question: str,
        user_age: int = None,
        user_gender: str = None,
        user_name: str = None,
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ —Ä–∞—Å–∫–ª–∞–¥—É"""
        logger.info(f"üéØ Generating answer for question: {question}")

        try:
            spread_data = self._get_spread_data(spread_id, user_id)
            if not spread_data:
                return {
                    "success": False,
                    "text": None,
                    "model": None,
                    "error": f"Spread {spread_id} for user {user_id} not found",
                }

            cards_text = self._format_cards_text(spread_data)
            interpretation_text = spread_data.get(
                "interpretation", "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞"
            )
            category = spread_data.get("category", "–æ–±—â–∞—è —Ç–µ–º–∞")
            spread_type = spread_data.get("spread_type", "unknown")

            profile_context = build_profile_context(
                user_age=user_age, user_gender=user_gender, user_name=user_name
            )

            prompt = build_question_answer_prompt(
                spread_type=spread_type,
                category=category,
                cards_text=cards_text,
                interpretation_text=interpretation_text,
                question=question,
                profile_context=profile_context,
            )

            preferred_model = self._get_preferred_model(user_id)
            models_to_try = self.model_list.copy()

            if preferred_model and preferred_model in models_to_try:
                models_to_try.remove(preferred_model)
                models_to_try.insert(0, preferred_model)

            for i, model in enumerate(models_to_try, 1):
                if self._is_model_in_cooldown(model):
                    continue

                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"üîÑ Trying model {i}/{len(models_to_try)} for question: {model}"
                    )

                result = await self._make_llm_request(model, prompt=prompt)

                if result["success"] and self._is_valid_interpretation(result["text"]):
                    logger.info(f"‚úÖ SUCCESS with model {model} for question")

                    if model != "deepseek/deepseek-r1:free":
                        self._set_preferred_model(user_id, model)

                    self._record_model_success(model)

                    cleaned_response = self._clean_response(result["text"])
                    final_response = self._clean_ai_response(cleaned_response)

                    return {
                        "success": True,
                        "text": final_response,
                        "model": model,
                        "error": None,
                    }
                else:
                    logger.warning(
                        f"‚ùå Model {model} failed for question: {result.get('error', 'Unknown error')}"
                    )
                    self._record_model_failure(model)
                    continue

            logger.error("‚ùå All models failed for question answering")
            return {
                "success": False,
                "text": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
                "model": None,
                "error": "All models failed",
            }

        except Exception as e:
            logger.error(f"‚ùå Critical error in generate_question_answer: {e}")
            return {
                "success": False,
                "text": None,
                "model": None,
                "error": f"Critical error: {str(e)}",
            }

    def _format_cards_text(self, spread_data: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫–∞—Ä—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        cards = spread_data.get("cards", [])
        if isinstance(cards, str):
            try:
                cards = json.loads(cards)
            except Exception:
                cards = []

        spread_type = spread_data.get("spread_type", "unknown")
        cards_text = ""

        if spread_type == "1 –∫–∞—Ä—Ç–∞" or len(cards) == 1:
            card = cards[0] if cards else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"
            if isinstance(card, dict):
                card_name = card.get("name", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞")
            else:
                card_name = str(card)
            cards_text = f"‚Ä¢ {card_name}"
        else:
            positions = ["–ü—Ä–æ—à–ª–æ–µ", "–ù–∞—Å—Ç–æ—è—â–µ–µ", "–ë—É–¥—É—â–µ–µ"]
            for i, card in enumerate(cards):
                if i < len(positions):
                    position = positions[i]
                else:
                    position = f"–ü–æ–∑–∏—Ü–∏—è {i + 1}"

                if isinstance(card, dict):
                    card_name = card.get("name", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞")
                else:
                    card_name = str(card)
                cards_text += f"‚Ä¢ {position}: {card_name}\n"

        return cards_text

    def _get_spread_data(self, spread_id: int, user_id: int):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ä–∞—Å–∫–ª–∞–¥–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            from src.user_database import UserDatabase

            user_db = UserDatabase()

            history = user_db.get_user_history_by_spread_id(user_id, spread_id)

            if history:
                return (
                    history[0]
                    if isinstance(history, list) and len(history) > 0
                    else history
                )
            else:
                return None

        except Exception as e:
            logger.error(
                f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Ä–∞—Å–∫–ª–∞–¥–∞ {spread_id} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}"
            )
            return None
