import aiohttp
import json
import logging
import asyncio
import re
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from .config import OPENROUTER_CONFIG

# ‚úÖ –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ì–ï–†–ê: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
logger = logging.getLogger(__name__)
logger.propagate = False  # ‚úÖ –ó–ê–ü–†–ï–¢ –î–£–ë–õ–ò–†–û–í–ê–ù–ò–Ø –õ–û–ì–û–í

class AIInterpreter:
    def __init__(self):
        self.api_key = OPENROUTER_CONFIG.api_key
        
        # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–û–†–Ø–î–û–ö –ú–û–î–ï–õ–ï–ô: meta-llama –ø–µ—Ä–≤–æ–π, deepseek –≤ –∫–æ–Ω—Ü–µ
        self.model_list = [
            "meta-llama/llama-3.3-70b-instruct",   # ‚úÖ –°–¢–ê–ë–ò–õ–¨–ù–ê–Ø –û–°–ù–û–í–ù–ê–Ø –ú–û–î–ï–õ–¨
            "google/gemma-2-9b-it:free",           # –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è
            "qwen/qwen-2-7b-instruct:free",        # –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è
            "microsoft/wizardlm-2-8x22b:free",     # –†–µ–∑–µ—Ä–≤–Ω–∞—è –º–æ—â–Ω–∞—è
            "deepseek/deepseek-r1:free"            # ‚úÖ –ü–ï–†–ï–ú–ï–©–ï–ù–ê –í –ö–û–ù–ï–¶ (–ø—Ä–æ–±–ª–µ–º—ã —Å rate-limit)
        ]
        
        # ‚úÖ –ó–ê–©–ò–¢–ê –û–¢ EDGE-CASES: –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —á—Ç–æ model_list –Ω–µ –ø—É—Å—Ç–æ–π
        self.model_list = self.model_list or []
        if not self.model_list:
            logger.error("üö® CRITICAL: model_list is empty! Using fallback meta-llama")
            self.model_list = ["meta-llama/llama-3.3-70b-instruct"]
        
        # ‚úÖ –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –í–´–°–û–ö–û–ì–û –£–†–û–í–ù–Ø: —Ç–æ–ª—å–∫–æ –ø–æ—Ä—è–¥–æ–∫ –º–æ–¥–µ–ª–µ–π
        model_names = [m.split('/')[-1] for m in self.model_list]
        logger.info(f"üîß AIInterpreter model_list order: {model_names}")
        
        self.base_url = OPENROUTER_CONFIG.base_url
        self.max_tokens = OPENROUTER_CONFIG.max_tokens
        self.temperature = 1.0
        
        # ‚úÖ PER-MODEL –¢–ê–ô–ú–ê–£–¢–´
        self.request_timeout = getattr(OPENROUTER_CONFIG, 'timeout', 60)  # –ë–∞–∑–æ–≤—ã–π —Ç–∞–π–º–∞—É—Ç 60 —Å–µ–∫—É–Ω–¥
        self.per_model_timeout = {
            "meta-llama/llama-3.3-70b-instruct": 90,  # 90 —Å–µ–∫—É–Ω–¥ –¥–ª—è —Ç—è–∂–µ–ª–æ–π –º–æ–¥–µ–ª–∏
            "microsoft/wizardlm-2-8x22b:free": 90,    # 90 —Å–µ–∫—É–Ω–¥ –¥–ª—è –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏
        }
        
        # ‚úÖ –£–°–û–í–ï–†–®–ï–ù–°–¢–í–û–í–ê–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø RETRY/BACKOFF
        self.max_retries = 2
        self.base_backoff = 1.5
        self.backoff_multiplier = 1.5
        self.max_backoff = 3.0  # ‚úÖ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ó–ê–î–ï–†–ñ–ö–ê 3 –°–ï–ö–£–ù–î–´
        
        logger.info(f"‚è±Ô∏è Request timeout: base={self.request_timeout}s, meta-llama=90s")
        logger.info(f"üîÑ Retry config: {self.max_retries} attempts, backoff: {self.base_backoff}‚Üí{self.max_backoff}s")
        
        # Circuit breaker state
        self._model_failures = {}
        self._model_cooldown_until = {}
        self._model_cooldown_duration = 300
        
        # Session cache for successful models
        self._preferred_models = {}
        self._preferred_model_ttl = 1800
        
        self._validate_parameters()
        self.prompt_cache = {}
        self.cache_size = 50
        
        logger.info(f"‚úÖ AI Interpreter initialized with {len(self.model_list)} models")

    def _validate_parameters(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        if not (0 <= self.temperature <= 2):
            logger.warning(f"‚ö†Ô∏è Invalid temperature {self.temperature}, clamping to 1.0")
            self.temperature = 1.0
        
        if self.max_tokens > 4000:
            logger.warning(f"‚ö†Ô∏è High max_tokens {self.max_tokens}, clamping to 4000")
            self.max_tokens = 4000

    def _get_request_timeout(self, model: str) -> int:
        """‚úÖ –ü–û–õ–£–ß–ï–ù–ò–ï –¢–ê–ô–ú–ê–£–¢–ê –î–õ–Ø –ö–û–ù–ö–†–ï–¢–ù–û–ô –ú–û–î–ï–õ–ò"""
        return self.per_model_timeout.get(model, self.request_timeout)

    def _calculate_backoff(self, attempt: int) -> float:
        """‚úÖ –†–ê–°–ß–ï–¢ BACKOFF –° –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï–ú –ú–ê–ö–°–ò–ú–£–ú–ê"""
        backoff = self.base_backoff * (self.backoff_multiplier ** attempt)
        return min(backoff, self.max_backoff)

    async def generate_interpretation(self,
                                    spread_type: str,
                                    cards: list,
                                    category: str,
                                    user_age: int = None,
                                    user_gender: str = None,
                                    user_name: str = None,
                                    user_id: Optional[int] = None,
                                    model: str = None) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ä–∞—Å–∫–ª–∞–¥–∞
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º: {success, text, model, error}
        """
        try:
            logger.info(f"üéØ Generating interpretation for {len(cards)} cards, category: {category}")
            
            # ‚úÖ DEBUG: –ª–æ–≥–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫ –º–æ–¥–µ–ª–µ–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if logger.isEnabledFor(logging.DEBUG):
                model_names = [m.split('/')[-1] for m in self.model_list]
                logger.debug(f"üîß Current model_list order: {model_names}")
            
            profile_context = self._build_profile_context(user_age, user_gender, user_name)
            spread_data = {
                'spread_type': spread_type,
                'cards': cards
            }
            
            prompt = self._build_prompt(spread_data, category, profile_context)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"üìù Prompt length: {len(prompt)} characters")
            
            # ‚úÖ –ï–°–õ–ò –ü–ï–†–ï–î–ê–ù–ê –ö–û–ù–ö–†–ï–¢–ù–ê–Ø –ú–û–î–ï–õ–¨ - –ò–°–ü–û–õ–¨–ó–£–ï–ú –¢–û–õ–¨–ö–û –ï–Å
            if model:
                logger.info(f"üéØ Using specific model: {model}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤ cooldown –ª–∏ –º–æ–¥–µ–ª—å
                if self._is_model_in_cooldown(model):
                    logger.warning(f"‚è∏Ô∏è Model {model} is in cooldown")
                    return {
                        "success": False,
                        "text": None,
                        "model": model,
                        "error": f"Model {model} is temporarily unavailable"
                    }
                
                result = await self._make_llm_request(
                    model=model,
                    spread_data=spread_data,
                    question_category=category,
                    profile_context=profile_context
                )
                
                if result["success"] and self._is_valid_interpretation(result["text"]):
                    logger.info(f"‚úÖ SUCCESS with model {model}")
                    self._record_model_success(model)
                    
                    cleaned_response = self._clean_response(result["text"])
                    final_response = self._clean_ai_response(cleaned_response)
                    
                    return {
                        "success": True,
                        "text": final_response,
                        "model": model,
                        "error": None
                    }
                else:
                    logger.warning(f"‚ùå Model {model} failed: {result.get('error', 'Unknown error')}")
                    self._record_model_failure(model)
                    return result
            
            # ‚úÖ –°–¢–ê–ù–î–ê–†–¢–ù–ê–Ø –õ–û–ì–ò–ö–ê –° –ö–≠–®–ï–ú –ò CIRCUIT BREAKER
            return await self._generate_with_fallback(
                spread_data=spread_data,
                category=category,
                profile_context=profile_context,
                user_id=user_id
            )
            
        except Exception as e:
            logger.error(f"‚ùå Unexpected error in generate_interpretation: {e}")
            return {
                "success": False,
                "text": None,
                "model": model,
                "error": f"Unexpected error: {str(e)}"
            }

    async def _generate_with_fallback(self, 
                                    spread_data: Dict, 
                                    category: str, 
                                    profile_context: str,
                                    user_id: Optional[int] = None) -> Dict[str, Any]:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å –∫—ç—à–µ–º –∏ circuit breaker"""
        preferred_model = self._get_preferred_model(user_id)
        models_to_try = self.model_list.copy()
        
        # ‚úÖ –ó–ê–ü–†–ï–¢ –ù–ê –ü–ï–†–ï–ú–ï–©–ï–ù–ò–ï DEEPSEEK –í –ù–ê–ß–ê–õ–û
        if preferred_model and preferred_model in models_to_try and preferred_model != "deepseek/deepseek-r1:free":
            models_to_try.remove(preferred_model)
            models_to_try.insert(0, preferred_model)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"üéØ Starting with preferred model: {preferred_model}")

        # –ü–µ—Ä–µ–±–æ—Ä –º–æ–¥–µ–ª–µ–π —Å circuit breaker
        for i, model in enumerate(models_to_try, 1):
            if self._is_model_in_cooldown(model):
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"‚è∏Ô∏è Skipping model in cooldown: {model}")
                continue
                
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"üîÑ Trying model {i}/{len(models_to_try)}: {model}")
            
            result = await self._make_llm_request(
                model=model,
                spread_data=spread_data,
                question_category=category,
                profile_context=profile_context
            )
            
            if result["success"] and self._is_valid_interpretation(result["text"]):
                logger.info(f"‚úÖ SUCCESS with model {model}")
                
                # ‚úÖ –ó–ê–ü–†–ï–¢ –ù–ê –ö–≠–®–ò–†–û–í–ê–ù–ò–ï DEEPSEEK
                if model != "deepseek/deepseek-r1:free":
                    self._set_preferred_model(user_id, model)
                
                self._record_model_success(model)
                
                cleaned_response = self._clean_response(result["text"])
                final_response = self._clean_ai_response(cleaned_response)
                
                return {
                    "success": True,
                    "text": final_response,
                    "model": model,
                    "error": None
                }
            else:
                logger.warning(f"‚ùå Model {model} failed: {result.get('error', 'Unknown error')}")
                self._record_model_failure(model)
                continue
        
        # –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
        logger.error("‚ùå All AI models failed to generate interpretation")
        fallback_text = self._generate_basic_interpretation(spread_data, category)
        
        return {
            "success": False,
            "text": fallback_text,
            "model": None,
            "error": "All models failed to generate valid interpretation"
        }

    async def _make_llm_request(self, model: str, prompt: Optional[str] = None, 
                               spread_data: Optional[Dict] = None, 
                               question_category: Optional[str] = None,
                               profile_context: str = "") -> Dict[str, Any]:
        """‚úÖ –£–°–û–í–ï–†–®–ï–ù–°–¢–í–û–í–ê–ù–ù–´–ô –ú–ï–¢–û–î –ó–ê–ü–†–û–°–ê"""
        
        if prompt is None:
            if spread_data is None or question_category is None:
                return {
                    "success": False,
                    "text": None,
                    "model": model,
                    "error": "Missing required parameters for prompt generation"
                }
            prompt = self._build_prompt(spread_data, question_category, profile_context)
        
        system_prompt = """
–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π —Ç–∞—Ä–æ–ª–æ–≥. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –¥–∞–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ä–∞—Å–∫–ª–∞–¥–æ–≤ –¢–∞—Ä–æ.

üö® –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê –Ø–ó–´–ö–ê:
1. –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï
2. –ù–ï –ò–°–ü–û–õ–¨–ó–£–ô –ê–ù–ì–õ–ò–ô–°–ö–ò–ï, –ö–ò–¢–ê–ô–°–ö–ò–ï –ò–õ–ò –î–†–£–ì–ò–ï –°–ò–ú–í–û–õ–´
3. –ü–ò–®–ò –ì–†–ê–ú–û–¢–ù–û –ò –ü–û–õ–ù–û–°–¢–¨–Æ –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï
4. –í—Å–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã –Ω–∞ —Ä—É—Å—Å–∫–∏–π
5. –ó–∞–ø—Ä–µ—â–µ–Ω—ã –ª—é–±—ã–µ –≤—Å—Ç–∞–≤–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–∞—Ö

–¢–≤–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ï–°–¢–ï–°–¢–í–ï–ù–ù–´–ú, –ì–†–ê–ú–û–¢–ù–´–ú –∏ –ü–û–õ–ï–ó–ù–´–ú - —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
"""
        
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "system",
                    "content": system_prompt.strip()
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "stream": False
        }
        
        payload = self._validate_payload(payload)
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://tarot-bot-luna.com",
            "X-Title": "Tarot Bot Luna"
        }
        
        # ‚úÖ –£–°–û–í–ï–†–®–ï–ù–°–¢–í–û–í–ê–ù–ù–´–ô BACKOFF –° –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï–ú
        for attempt in range(self.max_retries):
            start_time = time.time()
            try:
                timeout_seconds = self._get_request_timeout(model)
                timeout = aiohttp.ClientTimeout(total=timeout_seconds)
                
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"üì§ Sending request to {model}, attempt {attempt + 1}, timeout: {timeout_seconds}s")
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.base_url}/chat/completions",
                        headers=headers,
                        json=payload,
                        timeout=timeout
                    ) as response:
                        
                        end_time = time.time()
                        elapsed = end_time - start_time
                        response_headers = dict(response.headers)
                        
                        # ‚úÖ DEBUG: –ª–æ–≥–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç–∞
                        if logger.isEnabledFor(logging.DEBUG):
                            logger.debug(f"üì® Response (model={model}) status={response.status} time={elapsed:.1f}s")
                            logger.debug(f"üîß Response headers: {response_headers}")
                        
                        if response.status == 200:
                            raw_body = await response.text()
                            
                            # ‚úÖ DEBUG: –ª–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–µ —Ç–µ–ª–æ –æ—Ç–≤–µ—Ç–∞
                            if logger.isEnabledFor(logging.DEBUG):
                                logger.debug(f"üìÑ Raw response body (model={model}): {raw_body[:2000]!r}")
                            
                            try:
                                result = json.loads(raw_body)
                                interpretation = result['choices'][0]['message']['content'].strip()
                                
                                # ‚úÖ INFO: —Ç–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                                logger.info(f"‚úÖ SUCCESS: {model} responded in {elapsed:.1f}s, len={len(interpretation)}")
                                
                                return {
                                    "success": True,
                                    "text": interpretation,
                                    "model": model,
                                    "error": None
                                }
                            except (json.JSONDecodeError, KeyError, IndexError) as e:
                                logger.error(f"‚ùå Failed to parse response from {model}: {str(e)}")
                                return {
                                    "success": False,
                                    "text": None,
                                    "model": model,
                                    "error": f"Failed to parse API response: {str(e)}"
                                }
                        
                        else:
                            error_text = await response.text()
                            end_time = time.time()
                            elapsed = end_time - start_time
                            
                            logger.error(f"‚ùå API Error {response.status} for {model}: {error_text}")
                            
                            if response.status == 429:
                                retry_after = response_headers.get('Retry-After')
                                wait_time = self._calculate_backoff(attempt)
                                
                                if retry_after:
                                    try:
                                        wait_time = min(int(retry_after), self.max_backoff)
                                        if logger.isEnabledFor(logging.DEBUG):
                                            logger.debug(f"‚è∞ Using Retry-After header: {wait_time} seconds")
                                    except ValueError:
                                        logger.warning(f"‚ö†Ô∏è Invalid Retry-After header: {retry_after}")
                                
                                logger.warning(f"‚è≥ Rate limit hit for {model}. Waiting {wait_time:.1f} seconds...")
                                await asyncio.sleep(wait_time)
                                continue
                            
                            return {
                                "success": False,
                                "text": None,
                                "model": model,
                                "error": f"API returned status {response.status}: {error_text[:200]}"
                            }
                            
            except asyncio.TimeoutError:
                end_time = time.time()
                elapsed = end_time - start_time
                timeout_setting = self._get_request_timeout(model)
                
                logger.warning(f"‚è∞ Request timeout (model={model}) after {elapsed:.1f}s (timeout setting: {timeout_setting}s)")
                
                if attempt == self.max_retries - 1:
                    return {
                        "success": False,
                        "text": None,
                        "model": model,
                        "error": f"Timeout after {self.max_retries} attempts"
                    }
                
                wait_time = self._calculate_backoff(attempt)
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"‚è≥ Waiting {wait_time:.1f}s before retry after timeout...")
                await asyncio.sleep(wait_time)
                
            except Exception as e:
                end_time = time.time()
                elapsed = end_time - start_time
                
                logger.error(f"‚ùå Model {model} error on attempt {attempt + 1}: {str(e)}")
                
                if attempt == self.max_retries - 1:
                    return {
                        "success": False,
                        "text": None,
                        "model": model,
                        "error": f"Exception after {self.max_retries} attempts: {str(e)}"
                    }
                
                wait_time = self._calculate_backoff(attempt)
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"‚è≥ Waiting {wait_time:.1f}s before retry after exception...")
                await asyncio.sleep(wait_time)
        
        return {
            "success": False,
            "text": None,
            "model": model,
            "error": f"All {self.max_retries} attempts failed"
        }

    def _validate_payload(self, payload: Dict) -> Dict:
        """–ó–∞—â–∏—Ç–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è payload –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º"""
        validated_payload = payload.copy()
        
        temp = validated_payload.get('temperature', self.temperature)
        if not (0 <= temp <= 2):
            logger.warning(f"üö® Invalid temperature {temp}, clamping to 1.0")
            validated_payload['temperature'] = 1.0
        
        tokens = validated_payload.get('max_tokens', self.max_tokens)
        if tokens > 4000:
            logger.warning(f"üö® High max_tokens {tokens}, clamping to 4000")
            validated_payload['max_tokens'] = 4000
        
        return validated_payload

    def _is_model_in_cooldown(self, model: str) -> bool:
        """Circuit breaker: –ü—Ä–æ–≤–µ—Ä–∫–∞ cooldown –¥–ª—è –º–æ–¥–µ–ª–∏"""
        if model not in self._model_cooldown_until:
            return False
        
        cooldown_until = self._model_cooldown_until[model]
        if time.time() < cooldown_until:
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"üîí Model {model} in cooldown until {datetime.fromtimestamp(cooldown_until)}")
            return True
        
        del self._model_cooldown_until[model]
        if model in self._model_failures:
            del self._model_failures[model]
        return False

    def _record_model_failure(self, model: str):
        """Circuit breaker: –ó–∞–ø–∏—Å—å –Ω–µ—É–¥–∞—á–∏"""
        current_failures = self._model_failures.get(model, 0) + 1
        self._model_failures[model] = current_failures
        
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"üìâ Model {model} failure count: {current_failures}")
        
        if current_failures >= 3:
            cooldown_until = time.time() + self._model_cooldown_duration
            self._model_cooldown_until[model] = cooldown_until
            logger.warning(f"üö® Model {model} entering cooldown until {datetime.fromtimestamp(cooldown_until)}")

    def _record_model_success(self, model: str):
        """Circuit breaker: –°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –Ω–µ—É–¥–∞—á"""
        if model in self._model_failures:
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"üîÑ Resetting failure count for model {model}")
            del self._model_failures[model]

    def _get_preferred_model(self, user_id: Optional[int] = None) -> Optional[str]:
        """–ö—ç—à —É—Å–ø–µ—à–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if not user_id:
            return None
            
        if user_id in self._preferred_models:
            model, expiry_ts = self._preferred_models[user_id]
            if time.time() < expiry_ts and not self._is_model_in_cooldown(model):
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"üéØ Using preferred model {model} for user {user_id}")
                return model
            else:
                del self._preferred_models[user_id]
        
        return None

    def _set_preferred_model(self, user_id: Optional[int], model: str):
        """–ö—ç—à —É—Å–ø–µ—à–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if user_id:
            expiry_ts = time.time() + self._preferred_model_ttl
            self._preferred_models[user_id] = (model, expiry_ts)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"üíæ Cached preferred model {model} for user {user_id}")

    def _contains_english_text(self, text: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —è–∑—ã–∫–∞: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return False
        
        english_word_pattern = re.compile(r'\b[a-zA-Z]{3,}\b')
        english_words = english_word_pattern.findall(text)
        
        if len(english_words) >= 2:
            logger.warning(f"üö® Detected English words in response: {english_words[:3]}")
            return True
        
        return False

    def _is_valid_interpretation(self, interpretation: str) -> bool:
        """–£—Å–∏–ª–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —è–∑—ã–∫–∞"""
        if not interpretation or len(interpretation.strip()) < 50:
            logger.warning("‚ùå Invalid interpretation: too short")
            return False
        
        interpretation_lower = interpretation.lower()
        
        forbidden_phrases = [
            "provide me with more context", "could you please provide", 
            "what would you like me to do", "i need more information",
            "please provide", "tell me more", "–∫–∞–∫—É—é –∑–∞–¥–∞—á—É",
            "—á—Ç–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ", "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ", "—É—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞",
            "–∫–∞–∫ —Ç–∞—Ä–æ–ª–æ–≥, —è", "–≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ç–∞—Ä–æ–ª–æ–≥–∞", "—Å–æ–≥–ª–∞—Å–Ω–æ –∫–∞—Ä—Ç–∞–º —Ç–∞—Ä–æ"
        ]
        
        for phrase in forbidden_phrases:
            if phrase in interpretation_lower:
                logger.warning(f"‚ùå Invalid interpretation - contains forbidden phrase: {phrase}")
                return False
        
        if self._contains_english_text(interpretation):
            logger.warning("‚ùå Invalid interpretation - contains English text")
            return False
        
        if interpretation_lower.count('?') > 2:
            logger.warning("‚ùå Invalid interpretation - too many questions")
            return False
            
        if len(interpretation.split()) < 30:
            logger.warning("‚ùå Invalid interpretation - too few words")
            return False
        
        return True

    def _build_profile_context(self, user_age: int = None, user_gender: str = None, user_name: str = None):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        profile_context = ""
        
        if user_age or user_gender or user_name:
            profile_context = "–£—á–∏—Ç—ã–≤–∞–π —Å–ª–µ–¥—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ –ø—Ä–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, –Ω–æ –ù–ï —É–ø–æ–º–∏–Ω–∞–π –∏—Ö –ø—Ä—è–º–æ –≤ —Ç–µ–∫—Å—Ç–µ:\n"
            
            if user_name:
                profile_context += f"- –ò–º—è: {user_name}\n"
            
            if user_gender:
                gender_display = {
                    'male': '–º—É–∂—á–∏–Ω–∞',
                    'female': '–∂–µ–Ω—â–∏–Ω–∞', 
                    'other': '—á–µ–ª–æ–≤–µ–∫'
                }.get(user_gender, '—á–µ–ª–æ–≤–µ–∫')
                profile_context += f"- –ü–æ–ª: {gender_display}\n"
            
            if user_age:
                if user_age < 25:
                    age_group = "–º–æ–ª–æ–¥–æ–π"
                elif user_age < 35:
                    age_group = "–≤ —Ä–∞—Å—Ü–≤–µ—Ç–µ —Å–∏–ª" 
                elif user_age < 50:
                    age_group = "–∑—Ä–µ–ª—ã–π"
                else:
                    age_group = "–æ–ø—ã—Ç–Ω—ã–π"
                
                profile_context += f"- –í–æ–∑—Ä–∞—Å—Ç: {user_age} –ª–µ—Ç ({age_group})\n"
            
            profile_context += "\n–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, –Ω–æ –Ω–µ —É–∫–∞–∑—ã–≤–∞–π –∏—Ö —è–≤–Ω–æ.\n\n"
        
        return profile_context

    def _build_prompt(self, spread_data: Dict, question_category: str, profile_context: str = "") -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞"""
        cards = spread_data.get('cards', [])
        card_names = []
        for card in cards:
            if isinstance(card, dict):
                card_names.append(card['name'])
            else:
                card_names.append(card.name)
                
        cache_key = f"{question_category}:{','.join(card_names)}"
        if profile_context:
            cache_key += f":profile_{hash(profile_context)}"
        
        if cache_key in self.prompt_cache:
            return self.prompt_cache[cache_key]
        
        spread_type = spread_data.get('spread_type', 'unknown')
        spread_name = "–ö–∞—Ä—Ç–∞ –¥–Ω—è" if spread_type == '1 –∫–∞—Ä—Ç–∞' or len(cards) == 1 else "–†–∞—Å–∫–ª–∞–¥ –ü—Ä–æ—à–ª–æ–µ-–ù–∞—Å—Ç–æ—è—â–µ–µ-–ë—É–¥—É—â–µ–µ"
        
        cards_text = ""
        if spread_type == '1 –∫–∞—Ä—Ç–∞' or len(cards) == 1:
            card = cards[0]
            if isinstance(card, dict):
                card_name = card['name']
                is_reversed = card.get('is_reversed', False)
            else:
                card_name = card.name
                is_reversed = getattr(card, 'is_reversed', False)
            position_text = "–ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç–∞—è" if is_reversed else "–ø—Ä—è–º–∞—è"
            cards_text = f"‚Ä¢ {card_name} ({position_text})"
        else:
            positions = spread_data.get('positions', ["–ü—Ä–æ—à–ª–æ–µ", "–ù–∞—Å—Ç–æ—è—â–µ–µ", "–ë—É–¥—É—â–µ–µ"])
            for i, card in enumerate(cards):
                if isinstance(card, dict):
                    card_name = card['name']
                    is_reversed = card.get('is_reversed', False)
                else:
                    card_name = card.name
                    is_reversed = getattr(card, 'is_reversed', False)
                position = positions[i] if i < len(positions) else f"–ü–æ–∑–∏—Ü–∏—è {i+1}"
                position_text = "–ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç–∞—è" if is_reversed else "–ø—Ä—è–º–∞—è"
                cards_text += f"‚Ä¢ {position}: {card_name} ({position_text})\n"
    
        prompt = f"""
–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π —Ç–∞—Ä–æ–ª–æ–≥ —Å 20-–ª–µ—Ç–Ω–∏–º —Å—Ç–∞–∂–µ–º.

üö® –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê –Ø–ó–´–ö–ê:
‚Ä¢ –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï
‚Ä¢ –ù–ï –ò–°–ü–û–õ–¨–ó–£–ô –ê–ù–ì–õ–ò–ô–°–ö–ò–ï, –ö–ò–¢–ê–ô–°–ö–ò–ï –ò–õ–ò –î–†–£–ì–ò–ï –°–ò–ú–í–û–õ–´
‚Ä¢ –ü–ò–®–ò –ì–†–ê–ú–û–¢–ù–û –ò –ü–û–õ–ù–û–°–¢–¨–Æ –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï

{profile_context}

–¢–∏–ø —Ä–∞—Å–∫–ª–∞–¥–∞: {spread_name}
–ö–∞—Ç–µ–≥–æ—Ä–∏—è –≤–æ–ø—Ä–æ—Å–∞: {question_category}

–ö–∞—Ä—Ç—ã –≤ —Ä–∞—Å–∫–ª–∞–¥–µ:
{cards_text}

–ù–∞—á–Ω–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:
"""
        
        if len(self.prompt_cache) >= self.cache_size:
            oldest_key = next(iter(self.prompt_cache))
            del self.prompt_cache[oldest_key]
        
        self.prompt_cache[cache_key] = prompt
        return prompt

    def _clean_ai_response(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ AI-–æ—Ç–≤–µ—Ç–∞ –æ—Ç –∞–Ω–≥–ª–∏—Ü–∏–∑–º–æ–≤"""
        if not text:
            return text
        
        corrections = {
            'responsable': '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π',
            'stable': '—Å—Ç–∞–±–∏–ª—å–Ω—ã–π', 
            'energy': '—ç–Ω–µ—Ä–≥–∏—è',
            'card': '–∫–∞—Ä—Ç–∞',
            'spread': '—Ä–∞—Å–∫–ª–∞–¥',
            'upright': '–ø—Ä—è–º–∞—è',
            'reversed': '–ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç–∞—è',
            'tarot': '—Ç–∞—Ä–æ',
            'reading': '–≥–∞–¥–∞–Ω–∏–µ',
            'interpretation': '—Ç–æ–ª–∫–æ–≤–∞–Ω–∏–µ',
            'advice': '—Å–æ–≤–µ—Ç',
            'guidance': '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ',
            'message': '–ø–æ—Å–ª–∞–Ω–∏–µ',
        }
        
        for wrong, correct in corrections.items():
            text = text.replace(wrong, correct)
            text = text.replace(wrong.capitalize(), correct.capitalize())
        
        return text

    def _clean_response(self, response: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"""
        if not response:
            return response
            
        clean_phrases = [
            "–ö–æ–Ω–µ—á–Ω–æ, –≤–æ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:",
            "–í–æ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–∞—à–µ–≥–æ —Ä–∞—Å–∫–ª–∞–¥–∞:",
            "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–∞—Ä—Ç:",
            "–í–æ—Ç —á—Ç–æ –≥–æ–≤–æ—Ä—è—Ç –∫–∞—Ä—Ç–∞:",
            "–ö–∞—Ä—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç:",
            "–ù–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–µ–≥–æ —Ä–∞—Å–∫–ª–∞–¥–∞:",
        ]
        
        for phrase in clean_phrases:
            response = response.replace(phrase, "")
        
        response = response.strip()
        
        if len(response) > 2000:
            response = response[:2000] + "..."
        
        return response

    def _generate_basic_interpretation(self, spread_data: dict, question_category: str) -> str:
        """–õ–æ–∫–∞–ª—å–Ω—ã–π fallback"""
        cards = spread_data['cards']
        spread_type = spread_data['spread_type']
        
        if spread_type == '1 –∫–∞—Ä—Ç–∞':
            card = cards[0]
            
            if isinstance(card, dict):
                card_name = card['name']
                is_reversed = card.get('is_reversed', False)
            else:
                card_name = card.name
                is_reversed = getattr(card, 'is_reversed', False)
            
            interpretation = f"‚ú® {card_name} ({'–ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç–∞—è' if is_reversed else '–ø—Ä—è–º–∞—è'})\n\n"
            interpretation += f"–í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ {question_category} —ç—Ç–∞ –∫–∞—Ä—Ç–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –≤–∞–∂–Ω—ã–π –∞—Å–ø–µ–∫—Ç —Ç–≤–æ–µ–π –∂–∏–∑–Ω–∏."
            
        else:
            positions = spread_data.get('positions', ["–ü—Ä–æ—à–ª–æ–µ", "–ù–∞—Å—Ç–æ—è—â–µ–µ", "–ë—É–¥—É—â–µ–µ"])
            interpretation = "üîÆ –†–∞—Å–∫–ª–∞–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —ç—Ç–∞–ø–∞–º–∏:\n\n"
            
            for i, card in enumerate(cards):
                if isinstance(card, dict):
                    card_name = card['name']
                else:
                    card_name = card.name
                    
                position = positions[i] if i < len(positions) else f"–ü–æ–∑–∏—Ü–∏—è {i+1}"
                interpretation += f"‚Ä¢ {position}: {card_name}\n"
                
            interpretation += f"\n–û–±—â–∞—è —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è –≤ —Å—Ñ–µ—Ä–µ {question_category}."
        
        interpretation += "\n\n–î–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ø–æ–ø—Ä–æ–±—É–π —Å–¥–µ–ª–∞—Ç—å —Ä–∞—Å–∫–ª–∞–¥ –µ—â–µ —Ä–∞–∑."
        return interpretation

    async def generate_question_answer(self, spread_id: int, user_id: int, question: str, 
                                     user_age: int = None, user_gender: str = None, 
                                     user_name: str = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ —Ä–∞—Å–∫–ª–∞–¥—É"""
        logger.info(f"üéØ Generating answer for question: {question}")
        
        try:
            spread_data = self._get_spread_data(spread_id, user_id)
            if not spread_data:
                return {
                    "success": False,
                    "text": None,
                    "model": None,
                    "error": f"Spread {spread_id} for user {user_id} not found"
                }
            
            profile_context = self._build_profile_context(user_age, user_gender, user_name)
            cards_text = self._format_cards_text(spread_data)
            interpretation_text = spread_data.get('interpretation', '–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞')
            category = spread_data.get('category', '–æ–±—â–∞—è —Ç–µ–º–∞')
            spread_type = spread_data.get('spread_type', 'unknown')
            
            prompt = f"""
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —Ç–∞—Ä–æ–ª–æ–≥. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É —Ä–∞—Å–∫–ª–∞–¥—É.

{profile_context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{question}"

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞—Å–∫–ª–∞–¥–µ:
- –¢–∏–ø: {spread_type}
- –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}
- –ö–∞—Ä—Ç—ã: {cards_text}
- –ò—Å—Ö–æ–¥–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: {interpretation_text}

–û—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ):
"""
            
            preferred_model = self._get_preferred_model(user_id)
            models_to_try = self.model_list.copy()
            
            if preferred_model and preferred_model in models_to_try:
                models_to_try.remove(preferred_model)
                models_to_try.insert(0, preferred_model)

            for i, model in enumerate(models_to_try, 1):
                if self._is_model_in_cooldown(model):
                    continue
                    
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"üîÑ Trying model {i}/{len(models_to_try)} for question: {model}")
                
                result = await self._make_llm_request(model, prompt=prompt)
                
                if result["success"] and self._is_valid_interpretation(result["text"]):
                    logger.info(f"‚úÖ SUCCESS with model {model} for question")
                    
                    if model != "deepseek/deepseek-r1:free":
                        self._set_preferred_model(user_id, model)
                    
                    self._record_model_success(model)
                    
                    cleaned_response = self._clean_response(result["text"])
                    final_response = self._clean_ai_response(cleaned_response)
                    
                    return {
                        "success": True,
                        "text": final_response,
                        "model": model,
                        "error": None
                    }
                else:
                    logger.warning(f"‚ùå Model {model} failed for question: {result.get('error', 'Unknown error')}")
                    self._record_model_failure(model)
                    continue
            
            logger.error("‚ùå All models failed for question answering")
            return {
                "success": False,
                "text": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
                "model": None,
                "error": "All models failed"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Critical error in generate_question_answer: {e}")
            return {
                "success": False,
                "text": None,
                "model": None,
                "error": f"Critical error: {str(e)}"
            }

    def _format_cards_text(self, spread_data: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫–∞—Ä—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        cards = spread_data.get('cards', [])
        if isinstance(cards, str):
            try:
                cards = json.loads(cards)
            except:
                cards = []
        
        spread_type = spread_data.get('spread_type', 'unknown')
        cards_text = ""
        
        if spread_type == '1 –∫–∞—Ä—Ç–∞' or len(cards) == 1:
            card = cards[0] if cards else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞"
            if isinstance(card, dict):
                card_name = card.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞')
            else:
                card_name = str(card)
            cards_text = f"‚Ä¢ {card_name}"
        else:
            positions = ["–ü—Ä–æ—à–ª–æ–µ", "–ù–∞—Å—Ç–æ—è—â–µ–µ", "–ë—É–¥—É—â–µ–µ"]
            for i, card in enumerate(cards):
                if i < len(positions):
                    position = positions[i]
                else:
                    position = f"–ü–æ–∑–∏—Ü–∏—è {i+1}"
                
                if isinstance(card, dict):
                    card_name = card.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞')
                else:
                    card_name = str(card)
                cards_text += f"‚Ä¢ {position}: {card_name}\n"
        
        return cards_text

    def _get_spread_data(self, spread_id: int, user_id: int):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ä–∞—Å–∫–ª–∞–¥–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            from src.user_database import UserDatabase
            user_db = UserDatabase()
            
            history = user_db.get_user_history_by_spread_id(user_id, spread_id)
            
            if history:
                return history[0] if isinstance(history, list) and len(history) > 0 else history
            else:
                return None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Ä–∞—Å–∫–ª–∞–¥–∞ {spread_id} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
            return None